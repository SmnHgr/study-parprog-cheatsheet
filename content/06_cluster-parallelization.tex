\section{Cluster Parallelisierung}

\subsection{Message Passing Interface (MPI)}

Verteiltes Programmiermodell, basiert auf Actor/CSP-Prinzip, Jeder Prozess hat seine Identifikation (Rank) und arbeitet unabhängig in seinem Adressraum, starten und terminieren synchron $\rightarrow$ können untereinander kommunizieren (Nachrichten Senden/Empfangen), Synchronisation mit Barrieren


\begin{lstlisting}
#include <stdio.h> #include "mpi.h"
int main(int argc, char* argv[]){
  // MPI Initialisierung
  MPI_Init(&argc, &argv);

  // Prozess Identifikation
  int rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Prozess-Nummer innerhalb Gruppe

  int size;
  MPI_Comm_size(MPI_COMM_WORLD, &size); // Gesamtanzahl der Prozesse

  char name[MPI_MAX_PROCESSOR_NAME]; int len;
  MPI_Get_processor_name(name, &len);
  printf("MPI process %i", rank, name);

  // Senden/Empfangen eines Int-Wertes
  int tag = 0; // frei wählbare Nummer für Nachrichtenart >=0
  int sender = 0;
  if (rank == 0) {
    int value = rand(); int to;
    int length = 1;
    for (to = 1; to < size; to++) { MPI_Send(&value, length, MPI_INT, to, tag, MPI_COMM_WORLD); }
  } else {
    int value;
    MPI_Recv(&value, 1, MPI_INT, sender, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    printf("%i received by %i", value, rank); }

  // Warte, dass alle Prozesse Barriere erreichen
  MPI_Barrier(MPI_COMM_WORLD);

  // Aggregation von Teilresultaten (zwischen Prozessen)
  MPI_AllReduce(&value, &total, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD); // Jeder erhält Gesamtresultat als Rückgabewert, beinhaltet implizite Barriere/Broadcast über alle Prozesse

  MPI_Reduce(&value, &total, 1, MPI_INT, MPI_SUM, receiverRank, MPI_COMM_WORLD) // Nur Prozess receiverRank sieht das Gesamtresultat, kein Broadcast

  MPI_Finalize(); // MPI Finalisierung
  return 0; }
\end{lstlisting}

\textbf{Communicator} Gruppe von MPI-Prozessen \lstinline{MPI_COMM_WORLD} Communicator World, Gruppe aller Prozesse einer MPI-Programmausführung

\textbf{MPI Datentypen} \lstinline{MPI_CHAR} \lstinline{MPI_SHORT} \lstinline{MPI_INT} \lstinline{NPI_LONG} \lstinline{MPI_LONG_LONG} \lstinline{MPI_UNSIGNED} \lstinline{MPI_FLOAT}\lstinline{MPI_DOUBLE}

\textbf{Aggregatoren} \lstinline{MPI_MAX} \lstinline{MPI_MIN} \lstinline{MPI_SUM} \lstinline{MPI_PROD}

\textbf{Kommunikationskonzepte}
\textcolor{blue}{Broadcast} \lstinline{MPI_Bcast(&input, 1, MPI_INT, 0 /* sender */, MPI_COMM_WORLD)}
\textcolor{blue}{All to All} \lstinline{MPI_All(&input_array, size, MPI_INT, &output_array, size, MPI_INT, MPI_COMM_WORLD)}
\textcolor{blue}{Scatter} \lstinline{MPI_Scatter(&input_array, size, MPI_INT, &output_value, 1, MPI_INT, 0 /* sender */, MPI_COMM_WORLD)}
\textcolor{blue}{Gather} \lstinline{MPI_Gather(&input_value, 1, MPI_INT, &output_array, 1, MPI_INT, 0 /* receiver */, MPI_COMM_WORLD)}

\subsubsection{Monte Carlo - C}

\begin{lstlisting}
long count_hits(long trials) {
  long hits = 0, i;
  for (i = 0; i < trials; i++) {
    double x = (double)rand()/RAND_MAX;
    double y = (double)rand()/RAND_MAX;
    if (x * x + y * y <= 1) { hits++; } }
  return hits; }
int main(int argc, char* argv[]) {
  MPI_Init(&argc, &argv);

  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  srand(rank * 4711); // willkürlicher Seed für jeden Prozess
  long hits = count_hits(TRIALS / size) // Jeder Prozess rechnet Bruchteil
  long total;
  MPI_Reduce(&hits, &total, 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD); // Prozess 0 erhält Gesamtwert
  if (rank==0) { double pi = 4 * ((double)total / TRIALS); }

  long hits = count_hits(TRIALS);
  double pi = 4 * ((double)hits / TRIALS); }
\end{lstlisting}
