\section{GPU Parallelisierung}

\subsection{CUDA (Computer Unified Device Architecture, Nvidia)}

\textbf{Single Instruction Multiple Threads (SIMT)}

\textbf{CUDA Blocks}

\textbf{CUDA Ausführungsmodell} Gleicher Kernel wird von mehreren Threads ausgeführt (0 .. N-1) \textcolor{blue}{Thread} virtueller Skalarprozessor, Threads sind in Blöcke gruppiert \textcolor{blue}{Block} virtueller Multiprozessor, Ein Block = Gleicher SM, Threads können innerhalb Block interagieren, Blöcke müssen unabhängig sein (Run To Completion, Beliebige Ausführungsreihenfolge sequentiell/parallel, Blocks in Thread Pool)

\textbf{Datenaufteilung} Jede Kernel-Ausführung bestimmt seinen Datenteil, Programmierende modellieren Datenaufteilung selber

\begin{lstlisting}
/* CUDA Kernel -> GPU (Device) */
__global__ // Läuft auf GPU (Device)
void VectorAddKernel (float *A, float *B, float *C, int N){ // N = Anzahl Elemente
  int i = blockIdx.x * blockDim.x + threadIdx.x; // Index aus Blocknummer und -grösse sowie Thread Id innerhalb Bloc
  if (i < N) { C[i] = A[i] + B[i]; } } // Boundary Check (überflüssige Threads machen nichts)
\end{lstlisting}

\begin{lstlisting}
/* CUDA Ausführung - läuft auf CPU (Host) */
void CudaVectorAdd(float * A, float * B, float * C, int N){ // Läuft auf CPU (Host)
  size_t size = N * sizeof(float);
  float *d_A, *d_B, *d_C;

  // 1. Auf GPU allozieren (Alloziert Objekt in Device Global Memory)
  // Parameter: Pointer-Adresse, Size (Bytes)
  cudaMalloc(&d_A, size); cudaMalloc(&d_B, size); cudaMalloc(&d_C, size);

  // 2. Daten zu GPU transferieren (kopiert Speicher zwischen CPU/GPU)
  // Target Pointer, Source Pointer, Size, Destination
  cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
  cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);

  // 3. Kernel ausführen
  int blockSize = 1024; // oder threadsPerBlock / Anzahl Threads ???
  int gridSize = (N + blockSize - 1) / blockSize; // oder blocksPerGrid ???, N / blockSize aufgerundet (Anzahl Blöcke)
  VectorAddKernel<<<gridSize,blockSize>>>(d_A, d_B, d_C, N);

  // 4. Daten von GPU zu CPU transferieren (DeviceToHost)
  cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);

  // 5. Auf GPU deallozieren (dealloziert Objekt in Device Global Memory)
  cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } // Parameter: Pointer-Adresse

int main() {
  int N = 10000000; size_t size = N * sizeof(float);

  float *A = (float *)malloc(size); fillRandomArray(A, N);
  float *B = (float *)malloc(size); fillRandomArray(B, N);
  float *C = (float *)malloc(size);

  cudaVectorAdd(A, B, C, N);

  free(A); free(B); free(C);

  return 0; }
\end{lstlisting}

\subsubsection{Unified Memory}
Automatischer Transfer CPU/GPU (keine expliziten Memory Copies mehr)
\begin{lstlisting}
void cudaVectorAdd(float *A, float *B, float *C, int N) {
  int blockSize = 1024; int gridSize = (N + blockSize - 1) / blockSize;

  vectorAddKernel<<<gridSize, blockSize>>>(A, B, C, N);
  handleCudaError(cudaGetLastError());
  // Macht alles asynchrone wieder synchron, wartet auf Ende aller GPU Befehle
  handleCudaError(cudaDeviceSynchronize()); }

// Hilfsfunktion für Fehlerbehandlung (Wrapper für alle malloc, memcpy, free)
void handleCudaError(cudaError error) {
  if (error != cudaSuccess) {
  	fprintf(stderr, "CUDA: %s!\n", cudaGetErrorString(error));
  	exit(EXIT_FAILURE); } }

int main() {
  int N = 10000000; size_t size = N * sizeof(float);

  float *A; handleCudaError(cudaMallocManaged(&A, size)); fillRandomArray(A, N);
  float *B; handleCudaError(cudaMallocManaged(&B, size)); fillRandomArray(B, N);
  float *C; handleCudaError(cudaMallocManaged(&C, size));

  cudaVectorAdd(A, B, C, N);

  handleCudaError(cudaFree(A)); handleCudaError(cudaFree(B));
  handleCudaError(cudaFree(C));

  return 0; }
\end{lstlisting}

\textbf{3D Thread Hierarchie}

\begin{lstlisting}
const int C_ROWS = 1000; const int C_COLS = 2000; const int A_COLS = 1500;
const int A_ROWS = C_ROWS; const int B_ROWS = A_COLS; const int B_COLS = C_COLS;

const int BLOCK_SIZE = 32;
__global__
void matrixMultKernel(float *A, float *B, float *C) {
  int row = blockIdx.x * blockDim.x + threadIdx.x;
  int col = blockIdx.y * blockDim.y + threadIdx.y; // 2D Thread Hierarchie (Matrizen)
  if (row < C_ROWS && col < C_COLS) {
  	float sum = 0.0f; // 0
  	for (int k = 0; k < A_COLS; k++) { sum += A[row * A_COLS + k] * B[k * B_COLS + col];    } // äquivalent zu A[row, k] * B[k, col]
  	C[row * C_COLS + col] = sum; } } // äquivalent zu C[row, col]

cudaMatrixMult(float *A, float *B, float *C) {
  float *d_A, *d_B, *d_C;
  int sizeA = A_ROWS * A_COLS * sizeof(float);
  int sizeB = B_ROWS * B_COLS * sizeof(float);
  int sizeC = C_ROWS * C_COLS * sizeof(float);

  handleCudaError(cudaMalloc(&d_A, sizeA));
  handleCudaError(cudaMalloc(&d_B, sizeB));
  handleCudaError(cudaMalloc(&d_C, sizeC));

  handleCudaError(cudaMemcpy(d_A, A, sizeA, cudaMemcpyHostToDevice));
  handleCudaError(cudaMemcpy(d_B, B, sizeB, cudaMemcpyHostToDevice));

  dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE); // x-Achse, y-Achse -> 1024 (maximum block size)
  dim3 blocksPerGrid((C_ROWS + BLOCK_SIZE - 1) / BLOCK_SIZE, (C_COLS + BLOCK_SIZE - 1) / BLOCK_SIZE);
  matrixMultKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C);
  handleCudaError(cudaGetLastError());

  handleCudaError(cudaMemcpy(C, d_C, sizeC, cudaMemcpyDeviceToHost));

  handleCudaError(cudaFree(d_A)); handleCudaError(cudaFree(d_B));
  handleCudaError(cudaFree(d_C)); }
\end{lstlisting}

\subsection{Optimierungstechniken}

\subsubsection{Speichermodell}



\subsubsection{Synchronisation}

\begin{lstlisting}
__global__
void matrixMultKernel(float *A, float *B, float *C){

  __shared__ float Asub[TILE_SIZE][TILE_SIZE]; // Statische Array-Grösse allozieren
  __shared__ float Bsub[TILE_SIZE][TILE_SIZE]; // (Mehrdimensionalität bei statischer Grösse erlaubt)

  int tx = threadIdx.x, ty = threadIdx.y;
  int col = blockIdx.x * TILE_SIZE + tx;
  int row = blockIdx.y * TILE_SIZE + ty;
  int nofTiles = (A_COLS + TILE_SIZE - 1) / TILE_SIZE;

  float sum = 0.0
  for (int tile = 0; tile < nofTiles; tile++) {
    // Tile von A und B in Shared Memory lesen
    // Jeder Thread liest ein Element von jedem Tile
    if (row < A_ROWS && tile * TILE_SIZE + tx < A_COLS) {
      Asub[ty][tx] = A[row * A_COLS + tile * TILE_SIZE + tx]; }
    if (col < B_COLS && tile * TILE_SIZE + ty < B_ROWS) {
      Bsub[ty][tx] = B[(tile * TILE_SIZE + ty) * B_COLS + col];
	}
	__syncthreads(); // Cache is ready, Synchronisiert alle Threads innerhalb eines Blocks (notwendig für effizientes Caching)
	// Multipliziere Zeile von A-Tile mit Spalte von B-Tile aus dem Shared Memory
	if (row < C_ROWS && col < C_COLS) {
	  for (int ksub = 0; ksub < TILE_SIZE; ksub++) {
	    if (tile * TILE_SIZE + ksub < A_COLS) {
          sum += Asub[ty][ksub] * Bsub[ksub][tx]; } } }
    __syncthreads(); } // Einrückung ?????
	if (row < C_ROWS && col < C_COLS) {
	  C[row * C_COLS + col] = sum; }
\end{lstlisting}


\textbf{Warps}
Block wird intern in Wraps zerlegt (zu je 32 Threads), alle Threads führen gleiche Instruktion aus (SIMD), Verzweigungen (if, switch, while, do, for) werden abwechselnd ausgeführt (Divergenz)

\textbf{Divergenz}
Gibt es unterschiedliche Verzweigungen im selben Warp, führt SM Instruktion der einen Verzweigung durch (Bsp. if) $\rightarrow$ Die anderen Threads müssen warten. Dann wieder die andere Verzweigung (Bsp. else)


\subsubsection{(Memory) Coalescing}

\textbf{Burst} grösserer Datenblock (32 Bytes) wird mit einem kombinierten Zugriff auf konsekutiven Speicherbereich geladen (1 Transaktion)
