\section{GPU Parallelisierung - CUDA}

\textbf{Streaming Multiprocessor (SM)} erhält Instruktion, beinhaltet mehrere SPs

\textbf{Streaming Processors/Cores (SP)} führt Instruktionen aus

\textbf{Single Instruction Multiple Threads (SIMT)} alle Cores (SP) führen gleiche Instruktion (oder keine)/Kernel-Code aus \textcolor{red}{CPU/GPU kein gemeinsamer Hauptspeicher}

\textbf{CUDA Blocks}

\textbf{CUDA Ausführungsmodell} Gleicher Kernel wird von mehreren Threads ausgeführt (0 .. N-1) \textcolor{blue}{Thread} virtueller Skalarprozessor, Threads sind in Blöcke gruppiert \textcolor{blue}{Block} virtueller Multiprozessor, Ein Block = Gleicher SM, Threads können innerhalb Block interagieren, Blöcke müssen unabhängig sein (Run To Completion, Beliebige Ausführungsreihenfolge sequentiell/parallel, Blocks in Thread Pool)

\textbf{Datenaufteilung} Jede Kernel-Ausführung bestimmt seinen Datenteil, Programmierende modellieren Datenaufteilung selber

\begin{lstlisting}
/* CUDA Kernel -> GPU (Device) */
__global__ // Läuft auf GPU (Device)
void VectorAddKernel (float *A, float *B, float *C, int N){ // N = Anzahl Elemen
  // Index=Blocknummer * -grösse + ThreadId innerhalb Block
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  // Boundary Check: überflüssige Threads machen nichts
  if (i < N) { C[i] = A[i] + B[i]; } }
\end{lstlisting}

\begin{lstlisting}
/* CUDA Ausführung - läuft auf CPU (Host) */
void CudaVectorAdd(float * A, float * B, float * C, int N){//Läuft auf CPU (Host)
  size_t size = N * sizeof(float);
  float *d_A, *d_B, *d_C;

  // 1. Auf GPU allozieren (Alloziert Objekt in Device Global Memory)
  // Parameter: Pointer-Adresse, Size (Bytes)
  cudaMalloc(&d_A, size); cudaMalloc(&d_B, size); cudaMalloc(&d_C, size);

  // 2. Daten zu GPU transferieren (kopiert Speicher zwischen CPU/GPU)
  // Target Pointer, Source Pointer, Size, Destination
  cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
  cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);

  // 3. Kernel ausführen
  int blockSize = 1024; // oder threadsPerBlock / Anzahl Threads ???
  int gridSize = (N + blockSize - 1) / blockSize; // oder blocksPerGrid ???, N / blockSize aufgerundet (Anzahl Blöcke)
  VectorAddKernel<<<gridSize,blockSize>>>(d_A, d_B, d_C, N);

  // 4. Daten von GPU zu CPU transferieren (DeviceToHost)
  cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);

  // 5. Auf GPU deallozieren (dealloziert Objekt in Device Global Memory)
  cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } // Parameter: Pointer-Adresse

int main() {
  int N = 10000000; size_t size = N * sizeof(float);

  float *A = (float *)malloc(size); float *B = (float *)malloc(size);
  float *C = (float *)malloc(size);

  cudaVectorAdd(A, B, C, N);

  free(A); free(B); free(C);

  return 0; }
\end{lstlisting}

\subsection{Unified Memory}
Automatischer Transfer CPU/GPU (keine expliziten Memory Copies mehr)
\begin{lstlisting}
void cudaVectorAdd(float *A, float *B, float *C, int N) {
  int blockSize = 1024; int gridSize = (N + blockSize - 1) / blockSize;

  vectorAddKernel<<<gridSize, blockSize>>>(A, B, C, N);
  handleCudaError(cudaGetLastError());
  // Macht alles asynchrone wieder synchron, wartet auf Ende aller GPU Befehle
  handleCudaError(cudaDeviceSynchronize()); }

// Hilfsfunktion für Fehlerbehandlung (Wrapper für alle malloc, memcpy, free)
void handleCudaError(cudaError error) {
  if (error != cudaSuccess) {
  	fprintf(stderr, "CUDA: %s!\n", cudaGetErrorString(error));
  	exit(EXIT_FAILURE); } }

int main() {
  int N = 10000000; size_t size = N * sizeof(float);

  float *A; handleCudaError(cudaMallocManaged(&A, size)); fillRandomArray(A, N);
  float *B; handleCudaError(cudaMallocManaged(&B, size)); fillRandomArray(B, N);
  float *C; handleCudaError(cudaMallocManaged(&C, size));

  cudaVectorAdd(A, B, C, N);

  handleCudaError(cudaFree(A)); handleCudaError(cudaFree(B));
  handleCudaError(cudaFree(C));

  return 0; }
\end{lstlisting}

\textbf{3D Thread Hierarchie (Matrizen, eigentlich 2D)}

\begin{lstlisting}
const int C_ROWS = 1000; const int C_COLS = 2000; const int A_COLS = 1500;
const int A_ROWS = C_ROWS; const int B_ROWS = A_COLS; const int B_COLS = C_COLS;

const int BLOCK_SIZE = 32; // always
__global__
void matrixMultKernel(float *A, float *B, float *C) {
  int row = blockIdx.x * blockDim.x + threadIdx.x;
  int col = blockIdx.y * blockDim.y + threadIdx.y;
  if (row < C_ROWS && col < C_COLS) { // Boundary check
  	float sum = 0.0f; // 0
  	for (int k = 0; k < A_COLS; k++) {
  	  // equals A[row, k] * B[k, col]
      sum += A[row * A_COLS + k] * B[k * B_COLS + col]; }
  	C[row * C_COLS + col] = sum; } } // equals C[row, col], offset in simple array

cudaMatrixMult(float *A, float *B, float *C) {
  float *d_A, *d_B, *d_C;
  int sizeA = A_ROWS * A_COLS * sizeof(float);
  int sizeB = B_ROWS * B_COLS * sizeof(float);
  int sizeC = C_ROWS * C_COLS * sizeof(float);

  handleCudaError(cudaMalloc(&d_A, sizeA));
  handleCudaError(cudaMalloc(&d_B, sizeB));
  handleCudaError(cudaMalloc(&d_C, sizeC));

  handleCudaError(cudaMemcpy(d_A, A, sizeA, cudaMemcpyHostToDevice));
  handleCudaError(cudaMemcpy(d_B, B, sizeB, cudaMemcpyHostToDevice));

  // x-Achse, y-Achse -> 1024
  dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE); // maximum block size
  dim3 blocksPerGrid((C_ROWS + BLOCK_SIZE - 1) / BLOCK_SIZE, (C_COLS + BLOCK_SIZE - 1) / BLOCK_SIZE); // maximum grid size
  matrixMultKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C);

  handleCudaError(cudaGetLastError());

  handleCudaError(cudaMemcpy(C, d_C, sizeC, cudaMemcpyDeviceToHost));

  handleCudaError(cudaFree(d_A)); handleCudaError(cudaFree(d_B));
  handleCudaError(cudaFree(d_C)); }
\end{lstlisting}

\subsection{Optimierungstechniken}

\textbf{Shared Memory (Cache)} Synchronisation, nur innerhalb gleichen Blocks sichtbar, \lstinline{__shared__} definiert Shared Memory (Speicher-Allokation), \lstinline{__syncthreads()} synchronisiert alle Threads innerhalb eines Blocks (notwendig für effizientes Caching) \textcolor{red}{möglichst nicht in if/else setzen} \textcolor{red}{nutzlos wenn jeder Thread auf ein anderes Element zugreift}

\textcolor{blue}{Tiled Transposing} Jeweils ein Tile mit Coalescing vom Device Memory ins Shared Memory einlesen. SyncThreads. Danach vom Shared Memory das Transponierte Element mit Coalescing zurück in Device Memory schreiben

\begin{lstlisting}
__global__
void matrixMultKernel(float *A, float *B, float *C){

  // Shared Memory
  __shared__ float Asub[TILE_SIZE][TILE_SIZE]; // Statische Array-Grösse allozieren
  __shared__ float Bsub[TILE_SIZE][TILE_SIZE]; // (Mehrdimensionalität bei statischer Grösse erlaubt)

  int tx = threadIdx.x, ty = threadIdx.y;
  int col = blockIdx.x * TILE_SIZE + tx;
  int row = blockIdx.y * TILE_SIZE + ty;
  int nofTiles = (A_COLS + TILE_SIZE - 1) / TILE_SIZE;

  float sum = 0.0
  for (int tile = 0; tile < nofTiles; tile++) {
    // Tile von A und B in Shared Memory lesen
    // Jeder Thread liest ein Element von jedem Tile
    if (row < A_ROWS && tile * TILE_SIZE + tx < A_COLS) {
      Asub[ty][tx] = A[row * A_COLS + tile * TILE_SIZE + tx]; }
    if (col < B_COLS && tile * TILE_SIZE + ty < B_ROWS) {
      Bsub[ty][tx] = B[(tile * TILE_SIZE + ty) * B_COLS + col];
    }
    __syncthreads(); // Cache is ready
    // Multipliziere Zeile von A-Tile mit Spalte von B-Tile aus dem Shared Memory
    if (row < C_ROWS && col < C_COLS) {
      for (int ksub = 0; ksub < TILE_SIZE; ksub++) {
        if (tile * TILE_SIZE + ksub < A_COLS) {
          sum += Asub[ty][ksub] * Bsub[ksub][tx]; } } }
    __syncthreads(); }
    if (row < C_ROWS && col < C_COLS) {
      C[row * C_COLS + col] = sum; }
\end{lstlisting}


\textbf{Warps}
Block wird intern in Wraps zerlegt (zu je 32 Threads), alle Threads führen gleiche Instruktion aus (SIMD), Verzweigungen (if, switch, while, do, for) werden abwechselnd ausgeführt (Divergenz) \textcolor{blue}{Pro Warp selbe Instruktion!}

\textbf{Divergenz}
Gibt es unterschiedliche Verzweigungen im selben Warp, führt SM Instruktion der einen Verzweigung durch (Bsp. if) $\rightarrow$ Die anderen Threads müssen warten. Dann wieder die andere Verzweigung (Bsp. else)


\textbf{(Memory) Coalescing} Speicherzugriffe \textcolor{blue}{Burst} grösserer Datenblock (32 Bytes) wird mit einem kombinierten Zugriff auf konsekutiven Speicherbereich geladen (1 Transaktion) \textcolor{green}{x/y bei row/col/Launch-Konfig vertauschen} \lstinline{data[(Ausdruck ohne threadIdx.x /* offset */) + threadIdx.x]} in Zugriff
